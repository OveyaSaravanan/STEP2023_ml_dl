# -*- coding: utf-8 -*-
"""Task_MLP_on_Mnist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17qL1uUzi9TEXygjD3rvvBu44BBiCaPeF

# HandsOn - MLP on MNIST dataset

â–¶ The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems.

â–¶ The MNIST data consists of 60,000 training images and 10,000 test images.

â–¶ Each image is a crude 28 x 28 (784 pixels) handwritten digit from "0" to "9." Each pixel value is a grayscale integer between 0 and 255.

**Import essential libraries**
"""

"""
keras.datasets :  holds a list of custom pre-built datasets for ready use
keras.utils : package that contains utils / helper functions for pre-processing data
"""

#Importing MNIST dataset
from keras.datasets import mnist
# The np_utils module provides utilities for converting data types and manipulating arrays.
from keras.utils import np_utils

# Commented out IPython magic to ensure Python compatibility.
"""
matplotlib : collection of functions that creates various types of plots and visualizations.
matplotlib inline :  to set the backend of matplotlib to inline, ensuring that plots are displayed within the notebook cells.
"""
import matplotlib.pyplot as plt
# %matplotlib inline
import numpy as np

"""**Load dataset**"""

"""
X_train : input data, which are images of handwritten digits
y_train : the corresponding labels, which are the actual digits represented in the images.
X_test  : the input data for testing
y_test  : the corresponding labels for the testing data.

"""

#load_data : unpacks the data into tuples : (X_train, y_train) and (X_test, y_test)
(X_train, y_train), (X_test, y_test) = mnist.load_data()

print(X_train[1].shape)

"""
.shape : helps to view the dimensions of the numpy array
"""

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

print("Number of training examples :", X_train.shape[0], "and each image is of shape (%d, %d)"%(X_train.shape[1], X_train.shape[2]))
print("Number of training examples :", X_test.shape[0], "and each image is of shape (%d, %d)"%(X_test.shape[1], X_test.shape[2]))

"""**Visualizing train and test images**"""

"""
plt.subplot() : divides the figure into a grid of subplots.
330 : shorthand notation for a 3x3 grid of subplots
1 + i : the position of the current subplot within the grid.
"""


for i in range(9):
 # define subplot
 plt.subplot(330 + 1 + i)
 # plot raw pixel data
 plt.imshow(X_train[i], cmap=plt.get_cmap('gray'))
# show the figure
plt.show()

"""**Pre-processing : Training data**

---

> Reshape array

> Normalize data

> Do labelling

"""

"""
Reshaping the input data from a 2D format to a 1D format

X_train.shape[0] : the number of samples in the training dataset.
X_train.shape[1] : the height of each image.
X_train.shape[2] : the width of each image.

rows = number of samples
columns = height * width (28 * 28 = 784)

"""

#convert 2d to 1d
X_train = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2])
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2])

"""
.shape : helps to view the dimensions of the input array
"""

print(X_train.shape)
print(X_test.shape)

print(X_train[0])

"""
Pixel scaling : the original range of 0 to 255 to a new range of 0 to 1.
This normalization is done by dividing each pixel value by the maximum value (255 in this case).

pixel values are typically represented as integers between 0 and 255,
where 0 represents black and 255 represents white.

"""

X_train = X_train/255
X_test = X_test/255

print(X_train[0])

"""**Pre-processing : Target values**

"""

print("Class label of first image :", y_train[0])

"""
One-hot encoding is a technique used to represent categorical data, such as class labels, as binary vectors.
to_categorical() from the np_utils module :
      Arguments :
          y_train  : original class labels
          10.      : total number of classes

"""

#One-hot encoding
Y_train = np_utils.to_categorical(y_train, 10)
Y_test = np_utils.to_categorical(y_test, 10)

"""
Each element of the vector represents the presence or absence of a specific class
  1 indicating the presence of the class
  0 indicating the absence
"""

print("After converting the output into a vector : ",Y_train[0])

"""**Building a basic Model**"""

"""

keras.model
  -> Sequential : Used to build models layer by layer.
                  provides a linear stack of layers, where each layer has exactly one input tensor and one output tensor.

keras.layers
  -> Dense      : represents a fully connected layer
  -> Activation : Functions that introduces non-linearity to the model

"""

from keras.models import Sequential
from keras.layers import Dense, Activation

"""
input_dim : number of features(columns) in the input data
output_dim : there are 10 possible output classes, corresponding to the digits 0 to 9.

"""

input_dim = X_train.shape[1] #784
output_dim = 10 #0-9

"""
model : an object used to build and train the model

model :

      - Sequential class - represents a linear stack of layers
      - Dense layer : a fully connected layer
          - softmax activation : used for multi-class classification problems.

      - model compile
"""

model = Sequential()
model.add(Dense(output_dim, input_dim=input_dim, activation='softmax'))
model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])

"""
fit() method trains the model on the given training data and labels

Arguments :
    - X_train    : input training data, which consists of the preprocessed images after converting them to a 1D format.
    - Y_train    : the target training data, which consists of the one-hot encoded labels corresponding to the training data.
    - batch_size : the number of samples processed at each iteration before updating the model's parameters.
    - epochs     : Iteration or number of times the entire training dataset will be passed through the model during training.
    - verbose    : shows the progress bar when training
    - validation_data : provides the testing data and labels to be used for validation during training.

"""

history = model.fit(X_train, Y_train, batch_size=128, epochs=20, verbose=1, validation_data=(X_test, Y_test))

"""**Basic model evaluation**"""

"""
evaluates the trained neural network model on the testing data (X_test and Y_test)
prints the test score and accuracy

"""
score = model.evaluate(X_test, Y_test, verbose=0)
print('Test score:', score[0])
print('Test accuracy:', score[1])

"""**Let's visualize the test results**"""

"""
image_index : random image (choose < 10000)

reshape(28, 28) : the flattened image is set back into a 2D format of 28x28 pixels

cmap='Greys' : image will be displayed in black and white.

pred[image_index] : retrieves the predicted probabilities for the image at the specified index

np.argmax() : index of the maximum value in the predicted probabilities array
              indicating the predicted class label.
"""

image_index = 120
plt.imshow(X_test[image_index].reshape(28, 28),cmap='Greys')
pred = model.predict(X_test)
print(pred[image_index])
print(np.argmax(pred[image_index]))

"""**Let's dive deep into MLP ðŸ’ª**"""

# lets define constants for readability
NB_EPOCHS = 20
BATCH_SIZE = 128

"""Model :  MLP + Sigmoid activation + SGDOptimizer"""

"""
model :

    Sequential : defines the MLP model
    dense_1 : has 512 neurons (specify the input shape)
    dense_2 : has 128 neurons (automatically takes the output shape of the previous layer as its input)

Summary : defines the model's architecture, showing the number of parameters and the shape of each layer's output.

"""


# Multilayer perceptron

model_sigmoid = Sequential()
model_sigmoid.add(Dense(512, activation='sigmoid', input_shape=(input_dim,)))
model_sigmoid.add(Dense(128, activation='sigmoid'))
model_sigmoid.add(Dense(output_dim, activation='softmax'))

model_sigmoid.summary()

model_sigmoid.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])
history = model_sigmoid.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCHS, verbose=1, validation_data=(X_test, Y_test))

"""
evaluates the trained neural network model on the testing data (X_test and Y_test)
prints the test score and accuracy

"""
score = model.evaluate(X_test, Y_test, verbose=0)
print('Test score:', score[0])
print('Test accuracy:', score[1])

"""
image_index : random image (choose < 10000)

reshape(28, 28) : the flattened image is set back into a 2D format of 28x28 pixels

cmap='Greys' : image will be displayed in black and white.

pred[image_index] : retrieves the predicted probabilities for the image at the specified index

np.argmax() : index of the maximum value in the predicted probabilities array
              indicating the predicted class label.
"""

image_index = 30
plt.imshow(X_test[image_index].reshape(28, 28),cmap='Greys')
pred = model_sigmoid.predict(X_test)
print(pred[image_index])
print(np.argmax(pred[image_index]))

"""# TASKS

1. MLP + Sigmoid activation + ADAM Optimizer
2. MLP + ReLu Activation + ADAM Optimizer
3. MLP + Sigmoid Activation + Batch Normalization

Create models with the above layers, estimate the performance of the model and make predictions
"""